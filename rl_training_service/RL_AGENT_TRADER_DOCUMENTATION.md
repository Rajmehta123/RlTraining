# Train your RL Agent Trader - Complete Documentation

## Table of Contents
1. [Overview](#overview)
2. [System Architecture](#system-architecture)
3. [Supported Algorithms](#supported-algorithms)
4. [Feature Extraction System](#feature-extraction-system)
5. [Training Environment](#training-environment)
6. [API Reference](#api-reference)
7. [Configuration Options](#configuration-options)
8. [Training Process](#training-process)
9. [Backtesting & Evaluation](#backtesting--evaluation)
10. [Model Persistence](#model-persistence)
11. [Usage Guide](#usage-guide)
12. [Technical Specifications](#technical-specifications)

---

## Overview

The **RL Agent Trader** is a production-ready reinforcement learning system for training autonomous trading agents on stock market data. It combines state-of-the-art deep reinforcement learning algorithms with comprehensive financial feature engineering to create intelligent trading strategies.

### What It Is

A complete end-to-end machine learning pipeline that:
- Downloads historical stock data automatically
- Extracts 40+ technical and fundamental features
- Trains deep RL models using PPO, Rainbow DQN, or IQN algorithms
- Backtests trained models against baseline strategies
- Provides real-time trading predictions
- Saves models for future inference

### Key Capabilities

- âœ… **Multi-Algorithm Support**: PPO, Rainbow DQN, IQN
- âœ… **Tiered Feature Extraction**: 4 tiers covering technical, fundamental, market, and alpha signals
- âœ… **Real-time Training Progress**: TQDM-style progress with detailed metrics
- âœ… **Comprehensive Backtesting**: Compare against buy-hold and SMA crossover strategies
- âœ… **Risk-Aware Training**: Sharpe ratio optimization, drawdown penalties
- âœ… **Production-Ready API**: RESTful Flask backend with CORS support
- âœ… **Model Persistence**: Save/load models with metadata
- âœ… **User Authentication**: Multi-user support with Supabase integration

---

## System Architecture

### Technology Stack

**Backend (Python)**
- `Flask 3.0+` - Web API framework
- `Stable-Baselines3 2.0+` - Deep RL library (PPO)
- `sb3-contrib 2.0+` - Additional algorithms (Rainbow DQN, IQN)
- `Gymnasium 0.29+` - Environment API
- `PyTorch 2.0+` - Deep learning framework
- `yfinance 0.2.28+` - Market data provider
- `pandas 2.0+` - Data manipulation
- `scikit-learn 1.3+` - Feature normalization

**Frontend (React/TypeScript)**
- `React 18` - UI framework
- `TypeScript` - Type safety
- `Recharts` - Chart visualization
- `shadcn/ui` - Component library
- `Tailwind CSS` - Styling

### Component Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Frontend (React)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ RLTrading.tsxâ”‚  â”‚ rlTraining   â”‚  â”‚   Charts &   â”‚      â”‚
â”‚  â”‚   (Page)     â”‚â—„â”€â”¤   Service    â”‚  â”‚  Widgets     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ REST API
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Backend (Flask API)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                      app.py                          â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚
â”‚  â”‚  â”‚   /train   â”‚  â”‚  /predict  â”‚  â”‚  /models   â”‚    â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Core Components                         â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ DataHandler  â”‚  â”‚   Trading    â”‚  â”‚    RL     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚   (Data +    â”‚  â”‚ Environment  â”‚  â”‚  Agents   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  Features)   â”‚  â”‚  (Gymnasium) â”‚  â”‚(PPO/DQN)  â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚  â”‚
â”‚  â”‚  â”‚   Tiered     â”‚  â”‚  Backtester  â”‚                â”‚  â”‚
â”‚  â”‚  â”‚   Features   â”‚  â”‚   (Metrics)  â”‚                â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    External Services                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   yfinance   â”‚  â”‚     FMP      â”‚  â”‚   Supabase   â”‚      â”‚
â”‚  â”‚ (Price Data) â”‚  â”‚(Fundamentals)â”‚  â”‚  (Storage)   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Supported Algorithms

### 1. PPO (Proximal Policy Optimization)

**Default algorithm** - Best for general-purpose trading.

**Key Features:**
- Custom attention-based neural network architecture
- Policy gradient method with clipped objective
- Adaptive KL penalty for stable learning
- On-policy learning with experience replay

**Network Architecture:**
```python
TradingNetworkV2(
  features â†’ [256, 256] â†’ MultiHeadAttention(4 heads) â†’ [128] â†’ [policy, value]
)
```

**When to Use:**
- General stock trading
- When you want stable, reliable training
- Moderate computational budget
- Need interpretable policies

**Hyperparameters:**
- Learning rate: 3e-4 (default)
- Batch size: 64
- n_epochs: 10
- Gamma: 0.99
- GAE lambda: 0.95
- Clip range: 0.2

### 2. Rainbow DQN

**Advanced Q-learning** - Best for discrete action spaces.

**Key Features:**
- Distributional RL (51 quantiles)
- Double Q-learning
- Prioritized experience replay
- Dueling network architecture
- N-step returns

**Network Architecture:**
```python
QRDQNPolicy(
  features â†’ [512, 512, 256] â†’ Dueling(advantage, value) â†’ Q-distribution(51 quantiles)
)
```

**When to Use:**
- Want to model risk/uncertainty
- Strong computational resources
- Prefer value-based methods
- Need sample efficiency

**Hyperparameters:**
- Learning rate: 1e-4
- Batch size: 128
- Buffer size: 100,000
- Target update: 1,000 steps
- Exploration: Îµ-greedy (1.0 â†’ 0.05)
- N-quantiles: 51

### 3. IQN (Implicit Quantile Networks)

**State-of-the-art distributional RL** - Best for risk-aware trading.

**Key Features:**
- Fully parameterized quantile distribution
- Risk-aware reward shaping
- Larger network capacity (1024 hidden units)
- Drawdown penalties
- Volatility penalties
- Sharpe ratio bonuses

**Network Architecture:**
```python
QRDQNPolicy(
  features â†’ [1024, 512, 256] â†’ Quantile Embedding â†’ Q-distribution(100 quantiles)
)
```

**When to Use:**
- Want risk-aware trading
- Need to model tail risks
- Have significant compute
- Optimize for Sharpe ratio

**Risk Shaping:**
```python
RiskAwareRewardShaper(
  risk_aversion=0.5,
  drawdown_penalty=2.0,
  sharpe_bonus=1.0
)
```

**Hyperparameters:**
- Learning rate: 5e-5
- Batch size: 128
- Buffer size: 100,000
- N-quantiles: 100
- Gradient steps: 2

### Algorithm Comparison

| Feature | PPO | Rainbow DQN | IQN |
|---------|-----|-------------|-----|
| **Type** | Policy Gradient | Value-based | Distributional |
| **Sample Efficiency** | Low | High | Very High |
| **Training Speed** | Fast | Medium | Slow |
| **Risk Awareness** | Medium | Medium | High |
| **Memory Usage** | Low | Medium | High |
| **Stability** | High | Medium | Medium |
| **Best For** | General Trading | Sample-limited | Risk-aware |

---

## Feature Extraction System

### Tiered Feature Architecture

The system uses a **4-tier feature extraction hierarchy** that balances predictive power with computational efficiency.

### Tier 1: Price/Technical Features (26 features)

**Coverage:** ~60% of predictive importance

**Categories:**

1. **Price Momentum** (4 features)
   - `return_1d` - Daily return
   - `return_5d` - 5-day return
   - `return_10d` - 10-day return
   - `return_20d` - 20-day (monthly) return

2. **Volatility** (3 features)
   - `volatility_5d` - 5-day annualized volatility
   - `volatility_20d` - 20-day annualized volatility
   - `atr_normalized` - ATR / Close price

3. **Moving Averages** (5 features)
   - `price_vs_sma20` - Price deviation from 20-day MA
   - `price_vs_sma50` - Price deviation from 50-day MA
   - `price_vs_sma200` - Price deviation from 200-day MA
   - `sma20_vs_sma50` - Short vs medium term trend
   - `sma50_vs_sma200` - Medium vs long term trend

4. **RSI** (3 features)
   - `rsi` - Normalized RSI (0-1)
   - `rsi_oversold` - Binary flag (RSI < 30)
   - `rsi_overbought` - Binary flag (RSI > 70)

5. **MACD** (2 features)
   - `macd_signal` - MACD > Signal (binary)
   - `macd_histogram` - Normalized histogram

6. **Volume** (3 features)
   - `volume_ratio` - Current / 20-day avg
   - `volume_trend` - 5-day / 20-day avg
   - `obv_trend` - On-balance volume 10-day change

7. **Price Patterns** (4 features)
   - `high_low_range` - Daily range / Close
   - `close_position` - Where close sits in day's range
   - `upper_shadow` - Upper wick / Close
   - `lower_shadow` - Lower wick / Close

8. **Bollinger Bands** (2 features)
   - `bb_position` - Position within bands (0-1)
   - `bb_width` - Band width / middle band

### Tier 2: Fundamental Features (11 features)

**Coverage:** ~25% of predictive importance

**Data Source:** Financial Modeling Prep (FMP) API

1. **Valuation** (3 features)
   - `pe_normalized` - P/E ratio normalized to 0-1
   - `pb_normalized` - Price-to-book normalized
   - `ps_normalized` - Price-to-sales normalized

2. **Quality** (4 features)
   - `roe` - Return on equity
   - `gross_margin` - Gross profit margin
   - `net_margin` - Net profit margin
   - `debt_to_equity` - Debt-to-equity ratio (0-1)

3. **Earnings** (2 features)
   - `earnings_surprise` - Actual vs estimated earnings
   - `earnings_momentum` - Earnings growth rate

4. **Insider Activity** (1 feature)
   - `insider_net_signal` - (Buys - Sells) / Total

5. **Additional Legacy** (1 feature)
   - `pe_ratio` - Raw P/E ratio (when available)

### Tier 3: Market Context Features (4 features)

**Coverage:** ~10% of predictive importance

1. **Market Regime** (2 features)
   - `market_return` - SPY 5-day return (or proxy)
   - `market_trend` - SPY trend direction (-1/+1)

2. **Sector** (1 feature)
   - `sector_relative_strength` - Stock vs sector performance

3. **Macro** (1 feature)
   - `rate_environment` - Interest rate proxy

### Tier 4: Alpha Signals (2 features)

**Coverage:** ~5% of predictive importance

1. **Analyst Data**
   - `analyst_revision` - Consensus rating changes

2. **Valuation**
   - `dcf_discount` - DCF fair value vs current price

### Feature Selection Recommendations

| Use Case | Recommended Tiers | Feature Count | Training Time |
|----------|------------------|---------------|---------------|
| **Quick Prototype** | [1] | ~26 | Fastest |
| **Balanced (Default)** | [1, 2] | ~37 | Medium |
| **Comprehensive** | [1, 2, 3] | ~41 | Slower |
| **Maximum** | [1, 2, 3, 4] | ~43 | Slowest |

### Feature Preprocessing

All features undergo:
1. **NaN Handling**: Forward-fill â†’ Backward-fill â†’ Zero-fill
2. **Outlier Clipping**: Clipped to [-10, 10]
3. **Normalization**: Robust scaling using IQR
   ```python
   normalized = (value - median) / IQR
   clipped = clip(normalized, -3, 3)
   ```

---

## Training Environment

### Gymnasium Environment Specification

**Class:** `TradingEnvironment`

**Observation Space:**
- Type: `Box(low=-inf, high=inf, shape=(n_features,))`
- Features: Selected tier features (26-43 dimensions)
- Normalized: Yes (robust scaling)

**Action Space:**
- Type: `Discrete(5)`
- Actions:
  - `0` - STRONG SELL (sell and go short)
  - `1` - SELL (reduce position)
  - `2` - HOLD (maintain position)
  - `3` - BUY (increase position)
  - `4` - STRONG BUY (buy aggressively)

**Reward Function:**

```python
reward = portfolio_return + alpha * sharpe_ratio - beta * drawdown
```

Where:
- `portfolio_return` - Daily portfolio value change
- `sharpe_ratio` - Rolling Sharpe ratio (20-day window)
- `drawdown` - Current drawdown from peak
- `alpha` - Sharpe bonus weight (default: 0.1)
- `beta` - Drawdown penalty weight (default: 0.2)

**Episode:**
- Length: Full dataset length (train/val/test split)
- Termination: When data ends
- Reset: Returns to start of data

**State Features:**
- Current position (-1.0 to 1.0)
- Unrealized P&L
- Portfolio value
- Days in position
- Plus all selected tier features

### Environment Configuration

```python
DEFAULT_ENV_CONFIG = {
    'initial_capital': 100000,      # Starting capital
    'transaction_cost': 0.001,      # 0.1% per trade
    'slippage': 0.0005,             # 0.05% slippage
    'max_position_size': 1.0,       # 100% of capital
    'reward_scaling': 1.0,          # Reward multiplier
}
```

### Position Management

- **Long positions**: 0 to +1.0 (0% to 100% of capital)
- **Short positions**: Currently disabled (can be enabled)
- **Position changes**: Gradual (0.25 increments)
- **Transaction costs**: Applied on every position change

### Episode Statistics

Tracked metrics:
- Total return (%)
- Sharpe ratio
- Sortino ratio
- Calmar ratio
- Maximum drawdown
- Win rate
- Total trades
- Average trade duration
- Final portfolio value

---

## API Reference

### Base URL
```
http://127.0.0.1:5001
```

### Endpoints

#### 1. Health Check
```http
GET /api/health
```

**Response:**
```json
{
  "status": "healthy",
  "timestamp": "2024-01-15T10:30:00"
}
```

#### 2. Start Training
```http
POST /api/train
```

**Request Body:**
```json
{
  "symbol": "AAPL",
  "user_id": "user123",
  "config": {
    "algorithm": "ppo",
    "years": 2,
    "feature_tiers": [1, 2],
    "use_tiered_features": true,
    "env_config": {
      "initial_capital": 100000,
      "transaction_cost": 0.001,
      "slippage": 0.0005
    },
    "training_config": {
      "total_timesteps": 100000,
      "learning_rate": 0.0003,
      "batch_size": 64,
      "use_attention": true
    }
  }
}
```

**Response:**
```json
{
  "job_id": "AAPL_20240115_103000",
  "status": "started",
  "message": "Training started for AAPL"
}
```

#### 3. Get Training Status
```http
GET /api/train/{job_id}/status
```

**Response:**
```json
{
  "job_id": "AAPL_20240115_103000",
  "status": "training",
  "progress": 45,
  "symbol": "AAPL",
  "algorithm": "ppo",
  "started_at": "2024-01-15T10:30:00",
  "current_metrics": {
    "current_step": 45000,
    "total_steps": 100000,
    "progress_pct": 45.0,
    "elapsed_seconds": 120.5,
    "remaining_seconds": 148.2,
    "fps": 374,
    "mean_return": 0.0234,
    "sharpe_ratio": 1.45,
    "rollout/ep_rew_mean": 156.2,
    "train/loss": 0.0234
  },
  "data_summary": {
    "symbol": "AAPL",
    "start_date": "2022-01-15",
    "end_date": "2024-01-15",
    "total_rows": 504,
    "processed_rows": 480,
    "feature_count": 37,
    "has_fundamentals": true
  }
}
```

**Status Values:**
- `initializing` - Job created
- `loading_data` - Downloading price/fundamental data
- `preparing_environment` - Setting up Gym environment
- `building_model` - Creating neural network
- `training` - Active training
- `backtesting` - Running backtest
- `saving` - Saving model
- `completed` - Success
- `failed` - Error occurred
- `stopped` - User stopped

#### 4. Get Training Results
```http
GET /api/train/{job_id}/results
```

**Response:**
```json
{
  "job_id": "AAPL_20240115_103000",
  "results": {
    "metrics": {
      "agent": {
        "total_return": 0.2347,
        "annualized_return": 0.1234,
        "volatility": 0.1856,
        "sharpe_ratio": 1.45,
        "sortino_ratio": 2.12,
        "calmar_ratio": 1.89,
        "max_drawdown": -0.0823,
        "final_value": 123470,
        "win_rate": 0.58,
        "total_trades": 87,
        "n_days": 365,
        "n_weeks": 52,
        "n_months": 12,
        "start_date": "2023-01-15",
        "end_date": "2024-01-15"
      },
      "buy_hold": { /* Same structure */ },
      "sma_crossover": { /* Same structure */ }
    },
    "comparison": {
      "Strategy": ["RL Agent", "Buy & Hold", "SMA Cross"],
      "Total Return": [0.2347, 0.1856, 0.0945],
      "Sharpe Ratio": [1.45, 1.12, 0.87]
    },
    "plots": {
      "cumulative_returns": "data:image/png;base64,...",
      "drawdown": "data:image/png;base64,...",
      "returns_distribution": "data:image/png;base64,...",
      "rolling_sharpe": "data:image/png;base64,...",
      "portfolio_value": "data:image/png;base64,..."
    },
    "equity_curve": [
      {"date": "2023-01-15", "value": 100000, "return": 0.0},
      {"date": "2023-01-16", "value": 100234, "return": 0.00234}
    ],
    "trades": [
      {
        "step": 10,
        "date": "2023-01-25",
        "action": "BUY",
        "price": 145.23,
        "position": 0.5,
        "portfolio_value": 101234
      }
    ],
    "training_history": [
      {
        "step": 10000,
        "timestamp": "2024-01-15T10:35:00",
        "mean_return": 0.0156,
        "sharpe_ratio": 0.89,
        "max_drawdown": -0.0423,
        "win_rate": 0.52
      }
    ],
    "model_path": "/models/AAPL_20240115_103000/best_model.zip"
  }
}
```

#### 5. Stop Training
```http
POST /api/train/{job_id}/stop
```

**Response:**
```json
{
  "message": "Stop signal sent"
}
```

#### 6. Make Prediction
```http
POST /api/predict/{job_id}
```

**Request Body:**
```json
{
  "symbol": "AAPL"
}
```

**Response:**
```json
{
  "symbol": "AAPL",
  "recommendation": "BUY",
  "action": 3,
  "position": 0.75,
  "predicted_return": 0.0234,
  "metrics": {
    "total_return": 0.0234,
    "sharpe_ratio": 1.23,
    "max_drawdown": -0.0156
  }
}
```

#### 7. List Models
```http
GET /api/models
```

**Response:**
```json
{
  "models": [
    {
      "job_id": "AAPL_20240115_103000",
      "symbol": "AAPL",
      "created_at": "2024-01-15T10:30:00",
      "metrics": {
        "total_return": 0.2347,
        "sharpe_ratio": 1.45
      }
    }
  ]
}
```

#### 8. Delete Model
```http
DELETE /api/models/{job_id}
```

**Response:**
```json
{
  "message": "Model deleted"
}
```

---

## Configuration Options

### Training Configuration

```typescript
interface TrainingConfig {
  // Algorithm selection
  algorithm?: 'ppo' | 'rainbow_dqn' | 'iqn';

  // Data configuration
  years?: number;                    // 2-10 (minimum 2)
  feature_tiers?: number[];          // [1, 2, 3, 4]
  use_tiered_features?: boolean;     // true/false

  // Environment configuration
  env_config?: {
    initial_capital?: number;        // Default: 100000
    transaction_cost?: number;       // Default: 0.001 (0.1%)
    slippage?: number;               // Default: 0.0005 (0.05%)
    max_position_size?: number;      // Default: 1.0 (100%)
  };

  // Training configuration
  training_config?: {
    learning_rate?: number;          // Default: 3e-4 (PPO), 1e-4 (DQN)
    total_timesteps?: number;        // 50k-500k
    batch_size?: number;             // Default: 64 (PPO), 128 (DQN)
    n_epochs?: number;               // Default: 10 (PPO only)
    use_attention?: boolean;         // Default: true (PPO only)
  };
}
```

### Default Configurations

**PPO (Default):**
```python
{
  'learning_rate': 3e-4,
  'total_timesteps': 100000,
  'batch_size': 64,
  'n_epochs': 10,
  'gamma': 0.99,
  'gae_lambda': 0.95,
  'clip_range': 0.2,
  'vf_coef': 0.5,
  'ent_coef': 0.01,
  'use_attention': True
}
```

**Rainbow DQN:**
```python
{
  'learning_rate': 1e-4,
  'total_timesteps': 100000,
  'batch_size': 128,
  'buffer_size': 100000,
  'learning_starts': 10000,
  'target_update_interval': 1000,
  'train_freq': 4,
  'exploration_fraction': 0.3,
  'exploration_initial_eps': 1.0,
  'exploration_final_eps': 0.05
}
```

**IQN:**
```python
{
  'learning_rate': 5e-5,
  'total_timesteps': 100000,
  'batch_size': 128,
  'buffer_size': 100000,
  'learning_starts': 10000,
  'target_update_interval': 1000,
  'train_freq': 4,
  'gradient_steps': 2,
  'exploration_fraction': 0.3
}
```

---

## Training Process

### Workflow

```
1. DATA LOADING (5% complete)
   â”œâ”€â”€ Download OHLCV from yfinance
   â”œâ”€â”€ Download fundamentals from FMP
   â””â”€â”€ Validate data integrity

2. FEATURE ENGINEERING (15% complete)
   â”œâ”€â”€ Calculate Tier 1 features (technical)
   â”œâ”€â”€ Calculate Tier 2 features (fundamental)
   â”œâ”€â”€ Calculate Tier 3 features (market)
   â”œâ”€â”€ Calculate Tier 4 features (alpha)
   â”œâ”€â”€ Handle NaN values
   â””â”€â”€ Normalize features (robust scaling)

3. DATA SPLITTING (18% complete)
   â”œâ”€â”€ Train: 70%
   â”œâ”€â”€ Validation: 15%
   â””â”€â”€ Test: 15%

4. ENVIRONMENT CREATION (20% complete)
   â”œâ”€â”€ Create train environment
   â”œâ”€â”€ Create validation environment
   â””â”€â”€ Wrap in VecNormalize

5. MODEL BUILDING (20% complete)
   â”œâ”€â”€ Initialize neural network
   â”œâ”€â”€ Set up optimizer
   â””â”€â”€ Configure callbacks

6. TRAINING (20% â†’ 85% complete)
   â”œâ”€â”€ Training loop
   â”œâ”€â”€ Periodic evaluation
   â”œâ”€â”€ Checkpoint saving
   â””â”€â”€ Progress callbacks
       â”œâ”€â”€ Current step / Total steps
       â”œâ”€â”€ FPS (frames per second)
       â”œâ”€â”€ Elapsed / Remaining time
       â”œâ”€â”€ Rollout metrics
       â”œâ”€â”€ Training loss metrics
       â””â”€â”€ Custom eval metrics

7. BACKTESTING (85% â†’ 95% complete)
   â”œâ”€â”€ Test RL agent
   â”œâ”€â”€ Test buy-hold baseline
   â”œâ”€â”€ Test SMA crossover baseline
   â””â”€â”€ Generate comparison plots

8. SAVING (95% â†’ 100% complete)
   â”œâ”€â”€ Save model weights
   â”œâ”€â”€ Save VecNormalize stats
   â”œâ”€â”€ Save metadata
   â””â”€â”€ Update database
```

### Training Metrics

**Real-time Metrics (Updated every 2 seconds):**

1. **Progress Metrics:**
   - Current step / Total steps
   - Progress percentage
   - Elapsed time (HH:MM:SS)
   - Remaining time (HH:MM:SS)
   - FPS (frames per second)
   - Iterations completed

2. **Rollout Metrics (PPO):**
   - `ep_len_mean` - Average episode length
   - `ep_rew_mean` - Average episode reward

3. **Time Metrics (PPO):**
   - `fps` - Frames per second
   - `iterations` - Number of iterations
   - `total_timesteps` - Total steps taken
   - `time_elapsed` - Wall clock time

4. **Training Metrics (PPO):**
   - `approx_kl` - KL divergence
   - `clip_fraction` - % of clipped ratios
   - `entropy_loss` - Policy entropy
   - `explained_variance` - Value function quality
   - `learning_rate` - Current LR
   - `loss` - Total loss
   - `policy_gradient_loss` - Policy loss
   - `value_loss` - Value function loss

5. **Evaluation Metrics (Custom):**
   - `mean_return` - Average return
   - `sharpe_ratio` - Risk-adjusted return
   - `max_drawdown` - Maximum drawdown
   - `win_rate` - % of profitable trades

### Training Duration Estimates

| Config | Timesteps | Duration (CPU) | Duration (GPU) |
|--------|-----------|----------------|----------------|
| Quick | 50,000 | ~2-3 min | ~1-2 min |
| Default | 100,000 | ~5-7 min | ~2-4 min |
| Medium | 250,000 | ~15-20 min | ~6-10 min |
| Long | 500,000 | ~30-40 min | ~12-20 min |

*Assumes: i5/i7 CPU or RTX 3060+ GPU, 2 years of data, Tiers [1,2]*

---

## Backtesting & Evaluation

### Baseline Strategies

**1. Buy & Hold:**
- Buy on day 1
- Hold until end
- No trading costs (for comparison)

**2. SMA Crossover:**
- Short MA: 20 days (adaptive)
- Long MA: 50 days (adaptive)
- Buy signal: Short crosses above long
- Sell signal: Short crosses below long
- Includes transaction costs

### Performance Metrics

**Return Metrics:**
- `total_return` - Total return (%)
- `annualized_return` - Annualized return (%)
- `final_value` - Final portfolio value

**Risk Metrics:**
- `volatility` - Annualized volatility
- `max_drawdown` - Maximum peak-to-trough decline
- `downside_deviation` - Volatility of negative returns

**Risk-Adjusted Metrics:**
- `sharpe_ratio` - (Return - RFR) / Volatility
  - \> 1.0 = Good
  - \> 2.0 = Very Good
  - \> 3.0 = Excellent
- `sortino_ratio` - Return / Downside deviation
- `calmar_ratio` - Return / Max drawdown

**Trading Metrics:**
- `win_rate` - % of profitable trades
- `total_trades` - Number of trades
- `avg_trade_duration` - Average holding period
- `profit_factor` - Total profits / Total losses

**Time Metrics:**
- `n_days` - Trading days in backtest
- `n_weeks` - Weeks in backtest
- `n_months` - Months in backtest
- `start_date` - Backtest start
- `end_date` - Backtest end

### Visualization Plots

1. **Cumulative Returns:**
   - Comparison of all strategies
   - Shows compounding effect
   - Logarithmic scale option

2. **Drawdown Chart:**
   - Shows underwater periods
   - Peak-to-trough declines
   - Recovery periods

3. **Returns Distribution:**
   - Histogram of daily returns
   - Shows skewness/kurtosis
   - Identifies outliers

4. **Rolling Sharpe Ratio:**
   - 20-day rolling Sharpe
   - Shows consistency
   - Identifies stable periods

5. **Portfolio Value:**
   - Portfolio value over time
   - Shows growth trajectory
   - Includes all strategies

6. **Metrics Heatmap:**
   - All metrics comparison
   - Color-coded performance
   - Easy visual comparison

### Success Criteria

**Minimum Requirements (Production-Ready):**
- âœ… Sharpe Ratio > 1.0
- âœ… Max Drawdown < -20%
- âœ… Win Rate > 50%
- âœ… Total Return > Buy & Hold

**Good Performance:**
- âœ… Sharpe Ratio > 1.5
- âœ… Max Drawdown < -15%
- âœ… Win Rate > 55%
- âœ… Total Return > 1.5x Buy & Hold

**Excellent Performance:**
- âœ… Sharpe Ratio > 2.0
- âœ… Max Drawdown < -10%
- âœ… Win Rate > 60%
- âœ… Total Return > 2x Buy & Hold

---

## Model Persistence

### Storage Structure

```
/models/
â””â”€â”€ {symbol}_{timestamp}/
    â”œâ”€â”€ best_model.zip           # Model weights
    â”œâ”€â”€ vec_normalize.pkl        # Normalization stats
    â”œâ”€â”€ metadata.json            # Training config
    â””â”€â”€ training_history.json    # Eval results
```

### Model Files

**1. best_model.zip:**
- Neural network weights
- Optimizer state
- Policy/value function parameters
- ~2-10 MB depending on architecture

**2. vec_normalize.pkl:**
- Running mean/std of observations
- Running mean/std of rewards
- Clipping bounds
- Essential for inference

**3. metadata.json:**
```json
{
  "symbol": "AAPL",
  "algorithm": "ppo",
  "feature_tiers": [1, 2],
  "feature_columns": ["return_1d", "rsi", ...],
  "env_config": { ... },
  "training_config": { ... },
  "trained_at": "2024-01-15T10:30:00",
  "total_timesteps": 100000,
  "final_metrics": { ... }
}
```

**4. training_history.json:**
- Evaluation checkpoints
- Metrics over time
- Used for plotting learning curves

### Database Storage (Supabase)

**Table: `rl_models`**
```sql
CREATE TABLE rl_models (
  id UUID PRIMARY KEY,
  user_id UUID REFERENCES auth.users,
  job_id VARCHAR(255) UNIQUE,
  symbol VARCHAR(10),
  algorithm VARCHAR(50),
  model_path VARCHAR(500),
  metrics JSONB,
  created_at TIMESTAMP
);
```

**Benefits:**
- Multi-user support
- Access control
- Metadata querying
- Cloud backup
- Sharing capabilities

---

## Usage Guide

### Frontend Interface

#### 1. Training Tab

**Inputs:**
- **Symbol:** Stock ticker (e.g., AAPL, TSLA)
- **Algorithm:** PPO / Rainbow DQN / IQN
- **Data Years:** 2-10 years (minimum 2)
- **Initial Capital:** $1K - $1M
- **Training Steps:** 50K - 500K
- **Feature Tiers:** Visual tier selector
  - Tier 1 (Price/Technical)
  - Tier 2 (Fundamentals)
  - Tier 3 (Market Context)
  - Tier 4 (Alpha Signals)
- **Use Attention:** Toggle (PPO only)

**Training Progress Display:**
```
TRAINING PROGRESS (45%)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 45%

[45000/100000 | 02:03 < 02:35 | 374 fps | it: 702]

â”Œâ”€ Rollout Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Episode Reward Mean: 156.2               â”‚
â”‚ Episode Length Mean: 245                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ Time Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FPS: 374.23                              â”‚
â”‚ Iterations: 702                          â”‚
â”‚ Total Timesteps: 45000                   â”‚
â”‚ Time Elapsed: 02:03                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ Train Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Loss: 0.0234                             â”‚
â”‚ Policy Loss: -0.0156                     â”‚
â”‚ Value Loss: 0.0123                       â”‚
â”‚ Entropy Loss: -0.0089                    â”‚
â”‚ Approx KL: 0.0045                        â”‚
â”‚ Clip Fraction: 0.12                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ Evaluation Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Mean Return: 2.34%                       â”‚
â”‚ Sharpe Ratio: 1.45                       â”‚
â”‚ Max Drawdown: -8.23%                     â”‚
â”‚ Win Rate: 58.3%                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2. Backtest Results Tab

**Metrics Table:**
| Strategy | Return | Sharpe | Max DD | Win Rate | Trades |
|----------|--------|--------|--------|----------|--------|
| RL Agent | 23.47% | 1.45 | -8.23% | 58.3% | 87 |
| Buy & Hold | 18.56% | 1.12 | -12.4% | N/A | 1 |
| SMA Cross | 9.45% | 0.87 | -15.2% | 52.1% | 43 |

**Backtest Period:**
- ğŸ“… Jan 15, 2023 â†’ Jan 15, 2024
- ğŸ“Š 365 days (52 weeks, 12 months)
- ğŸ’¼ 21 trades executed
- â±ï¸ Test period: 15% of total data

**Charts:**
- Cumulative returns comparison
- Drawdown over time
- Returns distribution
- Rolling Sharpe ratio
- Portfolio value evolution

#### 3. Trade History Tab

**Trade Log:**
| Date | Action | Price | Position | P&L | Portfolio |
|------|--------|-------|----------|-----|-----------|
| 2023-01-25 | BUY | $145.23 | 50% | +$234 | $101,234 |
| 2023-02-10 | STRONG BUY | $148.56 | 100% | +$1,456 | $103,890 |
| ... | ... | ... | ... | ... | ... |

**Export Options:**
- Download as CSV
- Copy to clipboard
- Filter by action type
- Sort by date/profit

#### 4. Saved Models Tab

**Model List:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AAPL - Jan 15, 2024 10:30 AM           â”‚
â”‚ Algorithm: PPO | Sharpe: 1.45           â”‚
â”‚ Return: 23.47% | Drawdown: -8.23%       â”‚
â”‚ [Predict] [Delete]                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Prediction:**
- Enter symbol (can be different from training)
- Get recommendation: STRONG SELL â†’ STRONG BUY
- See predicted return
- View metrics on recent data

### Python API Usage

```python
import requests

# 1. Start training
response = requests.post('http://localhost:5001/api/train', json={
    'symbol': 'AAPL',
    'config': {
        'algorithm': 'ppo',
        'years': 2,
        'feature_tiers': [1, 2],
        'training_config': {
            'total_timesteps': 100000
        }
    }
})
job_id = response.json()['job_id']

# 2. Poll status
import time
while True:
    status = requests.get(f'http://localhost:5001/api/train/{job_id}/status').json()
    print(f"Status: {status['status']} - {status['progress']}%")

    if status['status'] in ['completed', 'failed']:
        break

    time.sleep(2)

# 3. Get results
results = requests.get(f'http://localhost:5001/api/train/{job_id}/results').json()
print(f"Sharpe Ratio: {results['results']['metrics']['agent']['sharpe_ratio']}")

# 4. Make prediction
prediction = requests.post(f'http://localhost:5001/api/predict/{job_id}', json={
    'symbol': 'AAPL'
}).json()
print(f"Recommendation: {prediction['recommendation']}")
```

### Direct Python Usage

```python
from data_handler import DataHandler
from trading_environment import TradingEnvironment
from rl_agent import RLTradingAgent
from backtester import Backtester

# 1. Load data
handler = DataHandler(
    symbol='AAPL',
    fmp_api_key='your_key',
    years=2,
    feature_tiers=[1, 2],
    use_tiered_features=True
)
handler.load_data()
handler.load_fundamentals()
handler.calculate_features()
handler.normalize_features(method='robust')

# 2. Split data
splits = handler.split_data(train_ratio=0.7, val_ratio=0.15)

# 3. Create environment
env = TradingEnvironment(
    df=splits['train'],
    feature_columns=handler.feature_columns,
    initial_capital=100000
)

# 4. Train agent
agent = RLTradingAgent(env, {
    'total_timesteps': 100000,
    'learning_rate': 3e-4
})
agent.build_model()
result = agent.train(eval_env=val_env)

# 5. Backtest
test_env = TradingEnvironment(df=splits['test'], ...)
backtester = Backtester()
metrics = backtester.backtest_agent(agent, test_env, splits['test'])

# 6. Save
agent.save('my_model')
```

---

## Technical Specifications

### System Requirements

**Minimum:**
- Python 3.9+
- 8 GB RAM
- 2 GB free disk
- 2-core CPU
- Internet connection

**Recommended:**
- Python 3.10+
- 16 GB RAM
- 5 GB free disk
- 4+ core CPU / CUDA GPU
- Fast internet

### Dependencies

**Core:**
- torch >= 2.0.0
- stable-baselines3 >= 2.0.0
- sb3-contrib >= 2.0.0
- gymnasium >= 0.29.0

**Data:**
- yfinance >= 0.2.28
- pandas >= 2.0.0
- numpy >= 1.24.0

**API:**
- flask >= 3.0.0
- flask-cors >= 4.0.0
- gunicorn >= 21.0.0

**ML:**
- scikit-learn >= 1.3.0
- tensorboard >= 2.14.0

**Visualization:**
- matplotlib >= 3.7.0
- seaborn >= 0.12.0

### Performance Optimization

**1. GPU Acceleration:**
```python
# Automatically uses 
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

**2. Multiprocessing:**
```python
# Use vectorized environments
from stable_baselines3.common.vec_env import SubprocVecEnv
env = SubprocVecEnv([make_env] * 4)  # 4 parallel environments
```

**3. Data Caching:**
- yfinance data cached locally
- FMP API responses cached
- Feature calculations memoized

**4. Batch Processing:**
- Batch size tuning (64-128)
- Gradient accumulation
- Mixed precision training (FP16)

### Scalability

**Concurrent Training:**
- Multiple jobs in background threads
- Thread-safe job tracking
- Resource isolation

**Data Volume:**
- Supports 2-10 years of daily data
- ~500-2500 data points
- Handles missing data gracefully

**Model Storage:**
- Efficient model serialization
- Incremental checkpointing
- Automatic cleanup of old models

### Security

**API Security:**
- CORS configuration
- Input validation
- Rate limiting (planned)
- API key authentication (planned)

**Data Security:**
- Environment variables for API keys
- No sensitive data in logs
- Secure model storage paths

**User Isolation:**
- User-specific model storage
- Database-level access control
- Job ownership tracking

### Error Handling

**Graceful Degradation:**
- Fallback to legacy features if tiered extraction fails
- Continue training on validation errors
- Resume from checkpoints on crash

**Error Messages:**
- Detailed error descriptions
- Stack traces in logs
- User-friendly frontend errors

**Recovery:**
- Automatic retry on transient failures
- Checkpoint-based recovery
- Database transaction rollback

---

## Troubleshooting

### Common Issues

**1. "No data found for symbol"**
- Verify ticker symbol is correct
- Check internet connection
- Try a different date range

**2. "FMP API key invalid"**
- Set environment variable: `FMP_API_KEY`
- Check key hasn't expired
- Verify API quota not exceeded

**3. "Training stuck at 0%"**
- Check backend logs
- Verify GPU/CPU resources available
- Restart Flask service

**4. "Backtest shows 0 for SMA"**
- Increase years parameter (need > 1 year)
- Check data quality
- Verify enough data points

**5. "Model prediction fails"**
- Ensure model fully trained
- Check symbol data available
- Verify VecNormalize loaded

### Performance Tips

**1. Faster Training:**
- Use GPU if available
- Reduce total_timesteps
- Use fewer feature tiers
- Increase batch size

**2. Better Results:**
- Use more data (5-10 years)
- Include Tier 2 (fundamentals)
- Increase total_timesteps
- Try different algorithms

**3. Memory Issues:**
- Reduce buffer_size (DQN)
- Use smaller batch_size
- Reduce feature tiers
- Clear old models

---

## Future Enhancements

### Planned Features

**Short-term:**
- [ ] Real-time paper trading integration
- [ ] Email notifications on completion
- [ ] Mobile-responsive UI
- [ ] Model comparison dashboard
- [ ] Hyperparameter auto-tuning

**Medium-term:**
- [ ] Multi-asset portfolio optimization
- [ ] Options trading support
- [ ] News sentiment integration
- [ ] Alternative data sources
- [ ] Ensemble model support

**Long-term:**
- [ ] Live trading integration (with broker APIs)
- [ ] Advanced risk management
- [ ] Custom reward functions
- [ ] Transfer learning across stocks
- [ ] Explainable AI visualizations

### Research Directions

- [ ] Hierarchical RL for multi-timeframe trading
- [ ] Meta-learning for rapid adaptation
- [ ] Curiosity-driven exploration
- [ ] Model-based RL for sample efficiency
- [ ] Multi-agent market simulation

---

## Credits & License

**Built with:**
- Stable-Baselines3 (MIT License)
- OpenAI Gymnasium (MIT License)
- PyTorch (BSD License)
- Flask (BSD License)
- yfinance (Apache 2.0 License)

**Data Providers:**
- Yahoo Finance (market data)
- Financial Modeling Prep (fundamentals)

**Developed by:** FinVision Platform Team

**License:** MIT (or your license)

---

## Support & Contact

**Documentation:** [Link to full docs]
**GitHub:** [Link to repo]
**Issues:** [Link to issues]
**Discord:** [Link to community]
**Email:** support@example.com

---

**Last Updated:** January 2024
**Version:** 1.0.0
**Status:** Production Ready âœ…
