# RL Trading Agent - Quick Reference Card

## ğŸš€ Quick Start Commands

```bash
# Install
pip install -r requirements.txt

# Set API keys
export FMP_API_KEY="your_key"

# Run server
python app.py
# â†’ http://127.0.0.1:5001

# Train (Python)
import requests
requests.post('http://localhost:5001/api/train', json={
    'symbol': 'AAPL',
    'config': {
        'algorithm': 'ppo',
        'years': 2,
        'feature_tiers': [1, 2],
        'training_config': {'total_timesteps': 100000}
    }
})
```

---

## ğŸ“Š Algorithms Cheat Sheet

| Algorithm | Speed | Performance | Memory | Use When |
|-----------|-------|-------------|--------|----------|
| **PPO** | âš¡âš¡âš¡ | â­â­â­ | ğŸ’¾ | Default, fast, stable |
| **Rainbow DQN** | âš¡âš¡ | â­â­â­â­ | ğŸ’¾ğŸ’¾ | Sample-efficient |
| **IQN** | âš¡ | â­â­â­â­â­ | ğŸ’¾ğŸ’¾ğŸ’¾ | Risk-aware, max perf |

---

## ğŸ¯ Feature Tiers Summary

| Tier | Count | Type | Need API | Importance |
|------|-------|------|----------|------------|
| **1** | 26 | Technical | âŒ | â­â­â­â­â­ 60% |
| **2** | 11 | Fundamentals | âœ… FMP | â­â­â­â­ 25% |
| **3** | 4 | Market | âœ… | â­â­â­ 10% |
| **4** | 2 | Alpha | âœ… | â­â­ 5% |

---

## âš™ï¸ Recommended Configs

### ğŸƒ Quick (2-3 min)
```python
{
  'algorithm': 'ppo',
  'years': 2,
  'feature_tiers': [1],
  'training_config': {'total_timesteps': 50000}
}
# Expected: Sharpe 1.0-1.2, Drawdown -15% to -20%
```

### âš–ï¸ Balanced (5-7 min) â­ **DEFAULT**
```python
{
  'algorithm': 'ppo',
  'years': 2,
  'feature_tiers': [1, 2],
  'training_config': {'total_timesteps': 100000}
}
# Expected: Sharpe 1.2-1.5, Drawdown -10% to -15%
```

### ğŸš€ High Performance (30-40 min)
```python
{
  'algorithm': 'iqn',
  'years': 5,
  'feature_tiers': [1, 2, 3],
  'training_config': {'total_timesteps': 250000}
}
# Expected: Sharpe 1.5-2.0, Drawdown -6% to -10%
```

---

## ğŸ“¡ API Endpoints

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/api/health` | GET | Check service |
| `/api/train` | POST | Start training |
| `/api/train/{id}/status` | GET | Check progress |
| `/api/train/{id}/results` | GET | Get metrics |
| `/api/predict/{id}` | POST | Get signal |
| `/api/models` | GET | List models |

---

## ğŸ“ˆ Training Status Flow

```
initializing â†’ loading_data â†’ preparing_environment â†’
building_model â†’ training â†’ backtesting â†’ saving â†’ completed
                                                    â†“
                                                  failed
```

---

## ğŸ¯ Action Space

| Action | Value | Meaning |
|--------|-------|---------|
| 0 | STRONG SELL | Sell & short |
| 1 | SELL | Reduce position |
| 2 | HOLD | Maintain |
| 3 | BUY | Increase position |
| 4 | STRONG BUY | Max buy |

---

## ğŸ“Š Key Metrics

### Must-Have
- **Sharpe Ratio** > 1.0 (Good), > 1.5 (Great), > 2.0 (Excellent)
- **Max Drawdown** < -20% (OK), < -15% (Good), < -10% (Great)
- **Win Rate** > 50% (OK), > 55% (Good), > 60% (Great)

### Also Track
- Total Return (%)
- Sortino Ratio
- Calmar Ratio
- Total Trades
- Final Portfolio Value

---

## ğŸ”§ Hyperparameters

### PPO Defaults
```python
learning_rate: 3e-4
batch_size: 64
n_epochs: 10
gamma: 0.99
use_attention: True
```

### Rainbow DQN Defaults
```python
learning_rate: 1e-4
batch_size: 128
buffer_size: 100000
target_update: 1000
```

### IQN Defaults
```python
learning_rate: 5e-5
batch_size: 128
buffer_size: 100000
gradient_steps: 2
```

---

## ğŸ› Troubleshooting

| Problem | Solution |
|---------|----------|
| No data found | Check ticker symbol |
| Training stuck | Check logs, restart service |
| Low Sharpe | More steps, add Tier 2, try IQN |
| Out of memory | Reduce batch_size, fewer tiers |
| Slow training | Use GPU, reduce timesteps |

---

## ğŸ’¡ Best Practices

### DO âœ…
- Start with PPO + [1,2] + 100K
- Use 2-5 years of data
- Monitor training metrics
- Trust backtest results
- Try multiple algorithms

### DON'T âŒ
- Use < 2 years data
- Skip Tier 2 if possible
- Stop training early
- Overfit to training set
- Ignore max drawdown

---

## ğŸ“ File Structure

```
rl_training_service/
â”œâ”€â”€ app.py                 # API server
â”œâ”€â”€ config.py              # Settings
â”œâ”€â”€ data_handler.py        # Data + features
â”œâ”€â”€ feature_extractor.py   # Tiered features
â”œâ”€â”€ trading_environment.py # Gym env
â”œâ”€â”€ rl_agent.py           # PPO
â”œâ”€â”€ dqn_agents.py         # Rainbow/IQN
â”œâ”€â”€ backtester.py         # Evaluation
â””â”€â”€ requirements.txt      # Dependencies
```

---

## ğŸ“ When to Use Each Algorithm

### PPO - Your First Choice
- âœ… Just starting out
- âœ… Want fast results
- âœ… Testing different stocks
- âœ… Limited compute

### Rainbow DQN - For Efficiency
- âœ… Limited training data
- âœ… Want distributional RL
- âœ… Medium compute budget
- âœ… Research purposes

### IQN - For Production
- âœ… Care about risk
- âœ… Maximize Sharpe ratio
- âœ… Professional trading
- âœ… Have compute resources

---

## ğŸ“‰ Training Time Estimates

| Config | Steps | CPU | GPU | Sharpe |
|--------|-------|-----|-----|--------|
| Quick | 50K | 2-3m | 1-2m | 1.0-1.2 |
| Default | 100K | 5-7m | 2-4m | 1.2-1.5 |
| Medium | 250K | 15-20m | 6-10m | 1.4-1.8 |
| Long | 500K | 30-40m | 12-20m | 1.6-2.0 |

*i7 CPU / RTX 3060 GPU, 2 years data, Tiers [1,2]*

---

## ğŸ” Environment Variables

```bash
# Required for fundamentals
FMP_API_KEY="your_fmp_key"

# Optional (for model storage)
SUPABASE_URL="your_supabase_url"
SUPABASE_KEY="your_supabase_key"

# Service config
RL_SERVICE_PORT=5001
RL_DEBUG=False
```

---

## ğŸ“š Example Training Request

```json
{
  "symbol": "AAPL",
  "user_id": "user123",
  "config": {
    "algorithm": "ppo",
    "years": 2,
    "feature_tiers": [1, 2],
    "use_tiered_features": true,
    "env_config": {
      "initial_capital": 100000,
      "transaction_cost": 0.001,
      "slippage": 0.0005
    },
    "training_config": {
      "total_timesteps": 100000,
      "learning_rate": 0.0003,
      "use_attention": true
    }
  }
}
```

---

## ğŸ¯ Success Criteria

### Minimum (Production-Ready)
- âœ… Sharpe > 1.0
- âœ… Drawdown < -20%
- âœ… Win Rate > 50%
- âœ… Return > Buy & Hold

### Good
- âœ… Sharpe > 1.5
- âœ… Drawdown < -15%
- âœ… Win Rate > 55%
- âœ… Return > 1.5Ã— Buy & Hold

### Excellent
- âœ… Sharpe > 2.0
- âœ… Drawdown < -10%
- âœ… Win Rate > 60%
- âœ… Return > 2Ã— Buy & Hold

---

## ğŸ”„ Typical Workflow

```
1. Set API keys
2. Start service (python app.py)
3. Open frontend (http://localhost:3000)
4. Enter symbol (e.g., AAPL)
5. Select algorithm (PPO recommended)
6. Choose feature tiers ([1,2] recommended)
7. Set training steps (100K recommended)
8. Click "Start Training"
9. Monitor progress (updates every 2s)
10. View results in "Backtest Results" tab
11. Check trade history
12. Use model for predictions
```

---

## ğŸ†˜ Need Help?

- ğŸ“– Full Docs: `RL_AGENT_TRADER_DOCUMENTATION.md`
- ğŸ¯ Feature Guide: `FEATURE_GUIDE.md`
- ğŸ“ README: `README.md`
- ğŸ› Issues: GitHub Issues
- ğŸ“§ Email: support@example.com

---

## ğŸ”¥ Pro Tips

1. **Always include Tier 2** if you have FMP API key (+25% performance)
2. **Use attention network** for PPO (better pattern recognition)
3. **Monitor Sharpe during training** - should improve over time
4. **Train for 100K+ steps** minimum for convergence
5. **Use 3-5 years data** for better generalization
6. **Compare multiple algorithms** on same stock
7. **Trust backtest** more than training metrics
8. **IQN for Sharpe optimization** - best risk-adjusted returns
9. **GPU accelerates** training 2-3x
10. **Start simple** (PPO + [1,2] + 100K), iterate from there

---

**Version:** 1.0.0
**Last Updated:** January 2024
**Status:** âœ… Production Ready

**Print this page for quick reference!**
